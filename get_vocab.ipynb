{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/rosiezhao/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/rosiezhao/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/rosiezhao/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gensim\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import sent_tokenize, RegexpTokenizer\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "nltk.download('wordnet')\n",
    "import re\n",
    "\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rosiezhao/anaconda3/lib/python3.7/site-packages/smart_open/smart_open_lib.py:398: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
      "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
     ]
    }
   ],
   "source": [
    "model = Word2Vec.load(\"../Documents/AIForGood/ClimateTimes/w2v_all-the-news\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6743539\n",
      "0.2517513\n",
      "0.33636707\n"
     ]
    }
   ],
   "source": [
    "print(model.wv.similarity('climate_change', 'chinese_hoax')) #what even\n",
    "print(model.wv.similarity('climate_change', 'trump'))\n",
    "print(model.wv.similarity('climate_change', 'wildfire'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'heat_wave', 'dry_spell', 'mph_wind', 'lightning_strike', 'storm_surge', 'disaster', 'winter_weather', 'extreme_weather', 'subsidence', 'avalanche', 'rising_temperature', 'flash_flooding', 'forest_fire', 'wildfire', 'california_drought', 'irma', 'flood', 'earthquake', 'earthquake_tsunami', 'extreme_heat', 'heavy_rain', 'flooded', 'rainstorm', 'temblor', 'hurricane_sandy', 'devastation', 'disaster_hurricane', 'severe_drought', 'aftershock', 'cyclone', 'hurricane', 'hurricane_harvey', 'weather_pattern', 'patzert', 'coral_bleaching', 'hurricane_irma', 'natural_disaster', 'historic_drought', 'rainfall', 'storm', 'hurricane_matthew', 'fire', 'bark_beetle', 'drought_flood', 'drought', 'tornado', 'severe_thunderstorm', 'climatic', 'rain_snow', 'torrential_rain', 'devastating_earthquake', 'heavy_rainfall', 'strong_wind', 'flooding', 'mudslide', 'quake', 'flash_flood', 'powerful_storm', 'tsunami'}\n",
      "\n",
      "{'emission', 'greenhouse_gas', 'ice_sheet', 'glacier', 'climate', 'temperature_rising', 'earth_warming', 'planet_warming', 'climate_change', 'ecologically', 'dioxide', 'human_activity', 'heat_trapping', 'global_warming', 'carbon_dioxide', 'sea_level', 'west_antarctic', 'scientific_consensus', 'average_temperature', 'human_induced', 'methane_emission', 'warming_planet', 'methane', 'environmentally', 'atmosphere', 'carbon_emission', 'temperature_rise', 'denier', 'environment', 'reduce_emission', 'warming_temperature', 'ocean_temperature', 'climate_changing', 'ecosystem'}\n"
     ]
    }
   ],
   "source": [
    "#Generate list of vocab words\n",
    "def compute_vocab_bank(model, seed_words, num_similar=5):\n",
    "    most_similar=[word for word in seed_words]\n",
    "    for word in seed_words:\n",
    "        temp = model.wv.most_similar(word, topn=num_similar)\n",
    "        for new_word, _ in temp:\n",
    "            most_similar.append(new_word)\n",
    "    \n",
    "    #remove duplicates\n",
    "    new_vocab = set(most_similar)\n",
    "    return new_vocab\n",
    "\n",
    "extreme_weather_seed = [\n",
    "    'natural_disaster',\n",
    "    'extreme_weather',\n",
    "    'wildfire',\n",
    "    'heat_wave',\n",
    "    'drought',\n",
    "    'forest_fire',\n",
    "    'flood',\n",
    "    'flash_flood',\n",
    "    'hurricane',\n",
    "    'earthquake',\n",
    "    'tsunami',\n",
    "    'cyclone'\n",
    "]\n",
    "\n",
    "climate_seed = [\n",
    "    'climate_change',\n",
    "    'environment',\n",
    "    'global_warming',\n",
    "    'emission',\n",
    "    'carbon_dioxide',\n",
    "    'greenhouse_gas',\n",
    "    'temperature_rising',\n",
    "    'sea_level'\n",
    "]\n",
    "\n",
    "extreme_new_vocab = compute_vocab_bank(model, extreme_weather_seed)\n",
    "print(extreme_new_vocab)\n",
    "\n",
    "print()\n",
    "\n",
    "climate_new_vocab = compute_vocab_bank(model, climate_seed)\n",
    "print(climate_new_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (MAIS-202)",
   "language": "python",
   "name": ".mais-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
